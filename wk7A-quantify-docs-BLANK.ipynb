{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigations in which documents are key\n",
    "\n",
    "We might have in hand vast amounts of raw data from leaks, public records, web scrapes, FOIA requests, or whistleblowers. These could be emails, financial documents, contracts, social media posts, judicial judgments and discloser forms. \n",
    "\n",
    "Finding the buried story is another challenge. How do we read through thousands of pages to structure all this unstructured data.\n",
    "\n",
    "AI will put on the brakes if you upload massive amounts of documents. \n",
    "\n",
    "We'll learn how to use modern investigative techniques to discover patterns, anomalies, and insights that might otherwise go unnoticed.\n",
    "\n",
    "Here are a handful of examples:\n",
    "\n",
    "**ProPublica**: <a href =\"https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin\">Facebook (Still) Letting Housing Advertisers Exclude Users by Race</a>\n",
    "- This investigation uncovered how Facebook allowed advertisers to exclude users by race. Journalists used keyword searches to identify discriminatory practices in ad targeting.\n",
    "\n",
    "**The New York Times**: <a href=\"https://www.nytimes.com/2015/10/25/us/racial-disparity-traffic-stops-driving-black.html\">The Disproportionate Risks of Driving While Black</a>\n",
    "- NYT reporters analyzed police traffic stop data to expose racial disparities. They used keyword searches to identify patterns in the data, revealing that Black drivers were more likely to be stopped and searched.\n",
    "\n",
    "**ICIJ**: <a href=\"https://www.icij.org/investigations/panama-papers/\">THE PANAMA PAPERS</a>\n",
    "- This monumental collaborative investigation dug through 11.5 million leaked files **(2.6 terabytes of data)** to expose the offshore holdings of world political leaders, links to global scandals, and details of the hidden financial dealings of fraudsters, drug traffickers, billionaires, celebrities, sports stars and more.\n",
    "\n",
    "\n",
    "Today, we'll learn to quantify unstructured text with this challenge:\n",
    "\n",
    "#### Confession Judgments\n",
    "\n",
    "Confession Judgments have been around since the Middle Ages, but are especially common in New York State.\n",
    "\n",
    "This legal procedure is a signed agreement where someone admits they owe money and allows the creditor to get a court judgment without a lawsuit if they don’t pay. It saves time for creditors but can be risky for debtors because they give up their right to defend themselves in court.\n",
    "\n",
    "You scrape thousands of confession judgments from NYS court website and want to quantify how many have been signed by residents to deal with falling behind in rent or mortgage payments, or on utility bills. You also discover an interesting confession judgment and decide you want to quantify that too.\n",
    "\n",
    "## Steps we need to take:\n",
    "1. Acquisition, either through scraping, FOAI, or being given files by a source(s)\n",
    "2. Organzing\n",
    "3. Reading & understanding a few to uncover any patterns\n",
    "4. Structuring unstructured content to quantify values\n",
    "\n",
    "We'll work our way up to that challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv1F1HyA9zmX"
   },
   "source": [
    "# Download, Capture, Structure\n",
    "\n",
    "A few weeks ago we downloaded all **all** the files on <a href=\"https://sandeepmj.github.io/scrape-example-page/pages.html\">the demo site</a>. \n",
    "\n",
    "Let's say we have all the files <a href=\"https://drive.google.com/file/d/1yNfAK-a72oLFv_QxRbvDi3HCj-Gxd6T9/view?usp=sharing\">in this folder</a>. Download this folder and move it next to your ```.ipynb``` file. (Soon we'll learn how to do this programmatically.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing our files using ```glob```\n",
    "\n",
    "```glob``` does one thing and one thing only: it collects all files in a folder and places them in a list.\n",
    "\n",
    "\"glob\" all files in the main directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/801368_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_11_46_00.csv',\n",
       " 'practice_documents/800118_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_J.txt',\n",
       " 'practice_documents/801337_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_16_00.csv',\n",
       " 'practice_documents/text_doc_08.txt',\n",
       " 'practice_documents/text_doc_H.txt',\n",
       " 'practice_documents/text_doc_I.txt',\n",
       " 'practice_documents/text_doc_09.txt',\n",
       " 'practice_documents/800394_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/adolph-coors-2015.pdf',\n",
       " 'practice_documents/pdf_2.pdf',\n",
       " 'practice_documents/pdf_3.pdf',\n",
       " 'practice_documents/adolph-coors-2014.pdf',\n",
       " 'practice_documents/pdf_1.pdf',\n",
       " 'practice_documents/800166_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_11_31_00.csv',\n",
       " 'practice_documents/adolph-coors-2013.pdf',\n",
       " 'practice_documents/pdf_4.pdf',\n",
       " 'practice_documents/pdf_5.pdf',\n",
       " 'practice_documents/pdf_7.pdf',\n",
       " 'practice_documents/pdf_6.pdf',\n",
       " 'practice_documents/pdf_8.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_31_00.csv',\n",
       " 'practice_documents/pdf_9.pdf',\n",
       " 'practice_documents/read_sample2.txt',\n",
       " 'practice_documents/read_sample1.txt',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_01_00.csv',\n",
       " 'practice_documents/801366_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/800120_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_04.txt',\n",
       " 'practice_documents/text_doc_10.txt',\n",
       " 'practice_documents/text_doc_D.txt',\n",
       " 'practice_documents/801367_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_E.txt',\n",
       " 'practice_documents/text_doc_05.txt',\n",
       " 'practice_documents/pdf_10.pdf',\n",
       " 'practice_documents/800119_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/801293_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_07.txt',\n",
       " 'practice_documents/text_doc_G.txt',\n",
       " 'practice_documents/text_doc_F.txt',\n",
       " 'practice_documents/text_doc_06.txt',\n",
       " 'practice_documents/text_doc_02.txt',\n",
       " 'practice_documents/text_doc_B.txt',\n",
       " 'practice_documents/text_doc_C.txt',\n",
       " 'practice_documents/text_doc_03.txt',\n",
       " 'practice_documents/text_doc_01.txt',\n",
       " 'practice_documents/text_doc_A.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pull all files in the main directory\n",
    "\n",
    "glob.glob(\"practice_documents/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/801368_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/800118_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/801337_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/800394_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/adolph-coors-2015.pdf',\n",
       " 'practice_documents/pdf_2.pdf',\n",
       " 'practice_documents/pdf_3.pdf',\n",
       " 'practice_documents/adolph-coors-2014.pdf',\n",
       " 'practice_documents/pdf_1.pdf',\n",
       " 'practice_documents/800166_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/adolph-coors-2013.pdf',\n",
       " 'practice_documents/pdf_4.pdf',\n",
       " 'practice_documents/pdf_5.pdf',\n",
       " 'practice_documents/pdf_7.pdf',\n",
       " 'practice_documents/pdf_6.pdf',\n",
       " 'practice_documents/pdf_8.pdf',\n",
       " 'practice_documents/pdf_9.pdf',\n",
       " 'practice_documents/801366_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/800120_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/801367_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/pdf_10.pdf',\n",
       " 'practice_documents/800119_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/801293_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## capture only the pdfs\n",
    "\n",
    "glob.glob(\"practice_documents/*.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/text_doc_J.txt',\n",
       " 'practice_documents/text_doc_08.txt',\n",
       " 'practice_documents/text_doc_H.txt',\n",
       " 'practice_documents/text_doc_I.txt',\n",
       " 'practice_documents/text_doc_09.txt',\n",
       " 'practice_documents/read_sample2.txt',\n",
       " 'practice_documents/read_sample1.txt',\n",
       " 'practice_documents/text_doc_04.txt',\n",
       " 'practice_documents/text_doc_10.txt',\n",
       " 'practice_documents/text_doc_D.txt',\n",
       " 'practice_documents/text_doc_E.txt',\n",
       " 'practice_documents/text_doc_05.txt',\n",
       " 'practice_documents/text_doc_07.txt',\n",
       " 'practice_documents/text_doc_G.txt',\n",
       " 'practice_documents/text_doc_F.txt',\n",
       " 'practice_documents/text_doc_06.txt',\n",
       " 'practice_documents/text_doc_02.txt',\n",
       " 'practice_documents/text_doc_B.txt',\n",
       " 'practice_documents/text_doc_C.txt',\n",
       " 'practice_documents/text_doc_03.txt',\n",
       " 'practice_documents/text_doc_01.txt',\n",
       " 'practice_documents/text_doc_A.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## capture only the text files\n",
    "glob.glob(\"practice_documents/*txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/801368_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_11_46_00.csv',\n",
       " 'practice_documents/800118_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_J.txt',\n",
       " 'practice_documents/801337_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_16_00.csv',\n",
       " 'practice_documents/text_doc_08.txt',\n",
       " 'practice_documents/text_doc_H.txt',\n",
       " 'practice_documents/text_doc_I.txt',\n",
       " 'practice_documents/text_doc_09.txt',\n",
       " 'practice_documents/800394_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/adolph-coors-2015.pdf',\n",
       " 'practice_documents/pdf_2.pdf',\n",
       " 'practice_documents/pdf_3.pdf',\n",
       " 'practice_documents/adolph-coors-2014.pdf',\n",
       " 'practice_documents/pdf_1.pdf',\n",
       " 'practice_documents/800166_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_11_31_00.csv',\n",
       " 'practice_documents/adolph-coors-2013.pdf',\n",
       " 'practice_documents/pdf_4.pdf',\n",
       " 'practice_documents/pdf_5.pdf',\n",
       " 'practice_documents/pdf_7.pdf',\n",
       " 'practice_documents/pdf_6.pdf',\n",
       " 'practice_documents/pdf_8.pdf',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_31_00.csv',\n",
       " 'practice_documents/pdf_9.pdf',\n",
       " 'practice_documents/read_sample2.txt',\n",
       " 'practice_documents/read_sample1.txt',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_01_00.csv',\n",
       " 'practice_documents/801366_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/800120_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_04.txt',\n",
       " 'practice_documents/text_doc_10.txt',\n",
       " 'practice_documents/text_doc_D.txt',\n",
       " 'practice_documents/801367_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_E.txt',\n",
       " 'practice_documents/text_doc_05.txt',\n",
       " 'practice_documents/pdf_10.pdf',\n",
       " 'practice_documents/800119_2022_Marie_A_Cannon_Commi_v_Marie_A_Cannon_Commi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/801293_2022_Marie_Cannon_Commissi_v_Marie_Cannon_Commissi_CONFESSION_OF_JUDGM_1.pdf',\n",
       " 'practice_documents/text_doc_07.txt',\n",
       " 'practice_documents/text_doc_G.txt',\n",
       " 'practice_documents/text_doc_F.txt',\n",
       " 'practice_documents/text_doc_06.txt',\n",
       " 'practice_documents/text_doc_02.txt',\n",
       " 'practice_documents/text_doc_B.txt',\n",
       " 'practice_documents/text_doc_C.txt',\n",
       " 'practice_documents/text_doc_03.txt',\n",
       " 'practice_documents/text_doc_01.txt',\n",
       " 'practice_documents/text_doc_A.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## capture only the pdfs that have the words adolf coors in them\n",
    "\n",
    "glob.glob(\"practice_documents/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/fla_count_as_of_2020-08-19_time_11_46_00.csv',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_16_00.csv',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_11_31_00.csv',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_31_00.csv',\n",
       " 'practice_documents/fla_count_as_of_2020-08-19_time_12_01_00.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## files about florida\n",
    "glob.glob(\"practice_documents/*fla*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/text_doc_J.txt',\n",
       " 'practice_documents/text_doc_08.txt',\n",
       " 'practice_documents/text_doc_H.txt',\n",
       " 'practice_documents/text_doc_I.txt',\n",
       " 'practice_documents/text_doc_09.txt',\n",
       " 'practice_documents/text_doc_04.txt',\n",
       " 'practice_documents/text_doc_10.txt',\n",
       " 'practice_documents/text_doc_D.txt',\n",
       " 'practice_documents/text_doc_E.txt',\n",
       " 'practice_documents/text_doc_05.txt',\n",
       " 'practice_documents/text_doc_07.txt',\n",
       " 'practice_documents/text_doc_G.txt',\n",
       " 'practice_documents/text_doc_F.txt',\n",
       " 'practice_documents/text_doc_06.txt',\n",
       " 'practice_documents/text_doc_02.txt',\n",
       " 'practice_documents/text_doc_B.txt',\n",
       " 'practice_documents/text_doc_C.txt',\n",
       " 'practice_documents/text_doc_03.txt',\n",
       " 'practice_documents/text_doc_01.txt',\n",
       " 'practice_documents/text_doc_A.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finally, just the ones that say text-doc...and store in list called target_files\n",
    "\n",
    "glob.glob(\"practice_documents/*text_doc*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort for clarity\n",
    "\n",
    "use ```sorted()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice_documents/text_doc_01.txt',\n",
       " 'practice_documents/text_doc_02.txt',\n",
       " 'practice_documents/text_doc_03.txt',\n",
       " 'practice_documents/text_doc_04.txt',\n",
       " 'practice_documents/text_doc_05.txt',\n",
       " 'practice_documents/text_doc_06.txt',\n",
       " 'practice_documents/text_doc_07.txt',\n",
       " 'practice_documents/text_doc_08.txt',\n",
       " 'practice_documents/text_doc_09.txt',\n",
       " 'practice_documents/text_doc_10.txt',\n",
       " 'practice_documents/text_doc_A.txt',\n",
       " 'practice_documents/text_doc_B.txt',\n",
       " 'practice_documents/text_doc_C.txt',\n",
       " 'practice_documents/text_doc_D.txt',\n",
       " 'practice_documents/text_doc_E.txt',\n",
       " 'practice_documents/text_doc_F.txt',\n",
       " 'practice_documents/text_doc_G.txt',\n",
       " 'practice_documents/text_doc_H.txt',\n",
       " 'practice_documents/text_doc_I.txt',\n",
       " 'practice_documents/text_doc_J.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sorted list\n",
    "target_files = sorted(glob.glob(\"practice_documents/*text_doc*\"))\n",
    "target_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Structure\n",
    "\n",
    "Create a dataframe that holds:\n",
    "\n",
    "- name of the renter, \n",
    "- whether their lease of renewed or terminated,\n",
    "- the name of the source file.\n",
    "\n",
    "We'll do this two ways:\n",
    "\n",
    "1. by coding Python (when we want to keep our data confidential).\n",
    "\n",
    "2. by tapping the ChatGPT API (when it's publicly available data).\n",
    "\n",
    "\n",
    "### We want to keep it confidential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "## step one, open and read files\n",
    "for target_file in target_files:\n",
    "    with open(target_file, \"r\") as my_text:\n",
    "        print (type(my_text))\n",
    "    \n",
    " # io stands for input output, r is for read.        \n",
    "    \n",
    " # io stands for input output, r is for read.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.readlines()` is a Python file method that reads **all lines** from an open text file and returns them as a **list of strings**, where each string represents one line (including the newline character `\\n` at the end of each line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step one, open and read files\n",
    "text_list = []\n",
    "for target_file in target_files[:2]:\n",
    "    with open(target_file, \"r\") as my_text:\n",
    "        # print (type(my_text))\n",
    "        all_text = my_text.readlines()\n",
    "        text_list.append(all_text)\n",
    " # io stands for input output, r is for read.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_my_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_my_text\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_my_text' is not defined"
     ]
    }
   ],
   "source": [
    "all_my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now let's add logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "decision_list = []\n",
    "for target_file in target_files:\n",
    "    with open(target_file, \"r\") as my_text:\n",
    "        # print (type(my_text))\n",
    "        all_text = my_text.readlines()\n",
    "        client = all_text[0].replace(\"Client:\", \"\").strip()\n",
    "        print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show decision list\n",
    "decision_list = []\n",
    "for target_file in target_files:\n",
    "    with open(target_file, \"r\") as my_text:\n",
    "        # print (type(my_text))\n",
    "        all_text = my_text.readlines()\n",
    "        client = all_text[0].replace(\"Client:\", \"\").strip()\n",
    "        # print(client)\n",
    "        decision = all_text[2]\n",
    "\n",
    "        if \"renew\" in decision:\n",
    "            decision = \"renew\"\n",
    "        elif \"terminate\" in decision:\n",
    "            decision = \"terminate\"\n",
    "        else:\n",
    "            decision = \"FLAG\"\n",
    "        \n",
    "        # print(decision)\n",
    "        decision_list.append({\n",
    "            \"client\": client,\n",
    "            \"decision\": decision,\n",
    "            \"source\": target_file\n",
    "        })\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## turn into dataframe\n",
    "df = pd.DataFrame(decision_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT API\n",
    "\n",
    "These documents are publicly available, but we have thousands. We can use ChatGPT API to avoid messages like \"You can only upload 10 documents at a time.\"\n",
    "\n",
    "Let's use the ```ChatGPT``` API, but some meta concepts first:\n",
    "\n",
    "## 1. Get Your API Key\n",
    "\n",
    "An API key is like a password that lets your code access the AI service.\n",
    "\n",
    "**How to get it:**\n",
    "- Go to the AI provider's website\n",
    "- Create an account\n",
    "- Find the API keys section\n",
    "- Generate a new key\n",
    "- Copy it immediately (you'll only see it once)\n",
    "- I save all my keys in a password manager in one document call ```API Keys```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Using Your API Key\n",
    "\n",
    "**Keep it secret:**\n",
    "- Store it in a separate file (called `.env`)\n",
    "- Never put it directly in your code\n",
    "- Never share it with others\n",
    "- Never upload it to GitHub or email it\n",
    "\n",
    "**Why?**\n",
    "- Anyone with your key can use your account\n",
    "- You'll be charged for their usage\n",
    "- They could rack up thousands of dollars in charges\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Keep Track of Cost\n",
    "\n",
    "**AI APIs charge you for:**\n",
    "- Every request you make\n",
    "- The amount of text you send\n",
    "- The amount of text the AI generates\n",
    "\n",
    "**How to manage costs:**\n",
    "- Check the pricing before you start\n",
    "- Estimate your total cost: (number of files) × (cost per file)\n",
    "- Test on a small batch first (like 10 files)\n",
    "- Set up billing alerts\n",
    "- Monitor your usage regularly\n",
    "\n",
    "**Example:**\n",
    "If each document costs ```$0.01``` to process, then 5000 documents = $50\n",
    "\n",
    "## Survey class on their RAMS\n",
    "\n",
    "## 4. Don't Send Sensitive Information\n",
    "\n",
    "**Never send:**\n",
    "- Social Security Numbers\n",
    "- Credit card numbers  \n",
    "- Passwords\n",
    "- Medical records\n",
    "- Personal financial information\n",
    "- Confidential source memos, emails, etc.\n",
    "\n",
    "**Why?**\n",
    "- Your data gets sent to the AI company's servers\n",
    "- It may be stored temporarily\n",
    "- There's always a privacy risk\n",
    "- It may be used as training data\n",
    "- It may be subpoenaed\n",
    "\n",
    "**When in doubt:** Remove sensitive information first, or don't use the API.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Get your API key** - Your password to use the AI service\n",
    "2. **Protect it** - Keep it secret, never share or upload it\n",
    "3. **Watch costs** - Test small, estimate total, set alerts\n",
    "4. **Privacy first** - Don't send sensitive personal or confidential data\n",
    "\n",
    "\n",
    "We need a few things before we can do this:\n",
    "1. An ChatGPT API key\n",
    "2. A way to hide that key (you don't to use up your entire quota if someone stumbles on your key)\n",
    "\n",
    "#### How to get a ChatGPT key:\n",
    "\n",
    "1. **Go to the OpenAI platform:** https://platform.openai.com/api-keys\n",
    "\n",
    "2. **Log in** (or create) your OpenAI account.\n",
    "\n",
    "3. **Navigate to the API keys section** (sometimes under your profile icon → \"View API keys\").\n",
    "\n",
    "4. **Click \"Create new secret key\"** (or similar). A new key will be generated.\n",
    "\n",
    "5. **Copy and securely store the key immediately**, because you'll only see it once. If you lose it, you'll need to regenerate. I keep all my API keys in Dashlane in a document called ```API keys```.\n",
    "\n",
    "6. **Set the key secretly in your notebook:**\n",
    "\n",
    "We will use `python-dotenv`, a professional way to hide your API key used at Bloomberg and other places.\n",
    "\n",
    "```python\n",
    "     from dotenv import load_dotenv\n",
    "     load_dotenv()  # Reads from .env file \n",
    "     import os   ## to handle environment variables\n",
    "```\n",
    "You need to ```pip install dotenv``` first.\n",
    "\n",
    "\n",
    "7. Create a file named ```.env``` using VSCode. **Note:** once you save it and close it, you won't see it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from dotenv) (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import \n",
    "import os ## use it to secretely pull in our API key\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull secret API key into your notebook\n",
    "chatGPT_key = os.getenv(\"OPEN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(chatGPT_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-2.6.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.11.1-cp312-cp312-macosx_11_0_arm64.whl (315 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.11.1 openai-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Craig Newmark Graduate School of Journalism is located in New York City, specifically in the area of Midtown Manhattan.\n"
     ]
    }
   ],
   "source": [
    "# use the key you already loaded\n",
    "client = OpenAI(api_key = chatGPT_key)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = \"gpt-4o-2024-08-06\",\n",
    "    input= [{\"role\": \"user\", \"content\":\"Hello, where is the Newmark graduate school of journalism located?\"}]\n",
    "    \n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From OpenAI:\n",
    "\n",
    "#### OpenAI Models Overview (for Data Journalism – 2025)\n",
    "\n",
    "| Model | Ideal Use Cases | Strengths | Limitations | Notes |\n",
    "|--------|----------------|------------|--------------|-------|\n",
    "| **gpt-4o-2024-08-06** | Deep text analysis, summarization, document extraction, mixed media (text + images) | Fast, accurate, handles long documents (128k tokens), can analyze charts/images | Slightly higher cost than 3.5 models | Best overall model for newsroom analytics & investigative automation |\n",
    "| **gpt-4-turbo** | Structured data extraction, complex reasoning, policy analysis | Excellent at nuanced reasoning & tone analysis | Text-only | Great for in-depth narrative analysis or investigative topics |\n",
    "| **gpt-3.5-turbo-0125** | Quick categorization, entity extraction, keyword tagging | Fast & cheap, good for large-scale automation | Weaker reasoning, less factual consistency | Ideal for bulk processing thousands of records or articles |\n",
    "| **gpt-4o-mini** | Summarization, tagging, lightweight insight extraction | Very low latency and cost | Less detail in long reasoning | Best for high-volume, real-time dashboards |\n",
    "| **text-embedding-3-large** | Search, clustering, similarity, topic modeling | Generates high-quality vector embeddings | Not generative (no text output) | Combine with a vector DB (e.g., FAISS, Pinecone) for newsroom archive search |\n",
    "| **text-embedding-3-small** | Fast keyword and semantic search | Cheap and lightweight | Lower embedding quality | Great for smaller newsroom tools or prototypes |\n",
    "| **whisper-1** | Transcription of interviews, speeches, press conferences | Accurate multilingual transcription | Audio-only | Can auto-generate transcripts for podcasts or public meetings |\n",
    "| **gpt-4o-realtime-preview** | Live interviews, streaming Q&A, event monitoring | Responds to audio, text, or video in real time | Experimental API access | Future-facing for live analysis during debates or press events |\n",
    "\n",
    "\\*Prices change frequently — always confirm at [https://openai.com/api/pricing](https://openai.com/api/pricing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking it down:\n",
    "\n",
    "### Line 1: Getting ChatGPT's response text\n",
    "```python\n",
    "result = response.choices[0].message.content.strip()\n",
    "```\n",
    "\n",
    "- `response` = the full API response object from OpenAI\n",
    "- `.choices[0]` = get the first response (OpenAI can return multiple options, but we just want the first one)\n",
    "- `.message.content` = extract the actual text ChatGPT wrote, message is the container that holds the actual content\n",
    "- `.strip()` = remove any extra spaces/newlines from the beginning or end\n",
    "\n",
    "**Example:** `result` becomes `\"Pen Federal Credit Union|renew\"`\n",
    "\n",
    "---\n",
    "\n",
    "### Line 2: Splitting the text into two variables\n",
    "```python\n",
    "client_name, decision = result.split('|')\n",
    "```\n",
    "\n",
    "- `result.split('|')` = split the string at the pipe character `|`, creating a list with 2 items\n",
    "- `client_name, decision =` = unpack those 2 items into 2 separate variables\n",
    "\n",
    "**Example:**\n",
    "- `result.split('|')` → `[\"Pen Federal Credit Union\", \"renew\"]`\n",
    "- Then Python assigns:\n",
    "  - `client_name = \"Pen Federal Credit Union\"`\n",
    "  - `decision = \"renew\"`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1 of 20 \n",
      "processing 2 of 20 \n",
      "processing 3 of 20 \n",
      "processing 4 of 20 \n",
      "processing 5 of 20 \n",
      "processing 6 of 20 \n",
      "processing 7 of 20 \n",
      "processing 8 of 20 \n",
      "processing 9 of 20 \n",
      "processing 10 of 20 \n",
      "processing 11 of 20 \n",
      "processing 12 of 20 \n",
      "processing 13 of 20 \n",
      "processing 14 of 20 \n",
      "processing 15 of 20 \n",
      "processing 16 of 20 \n",
      "processing 17 of 20 \n",
      "processing 18 of 20 \n",
      "processing 19 of 20 \n",
      "processing 20 of 20 \n",
      "done processing!\n"
     ]
    }
   ],
   "source": [
    "decisions_list = []\n",
    "\n",
    "for i, target_file in enumerate(target_files, start = 1):\n",
    "    print(f\"processing {i} of {len(target_files)} \")\n",
    "    # open and read\n",
    "    with open(target_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Single API call asking for simple format\n",
    "    ## the next few lines just say hey, here's my prompt and the text i want you to look at.\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\", ## i, the human, am asking the question.\n",
    "            \"content\": f\"\"\"Extract the client name and decision from this document.\n",
    "Return ONLY in this format: client_name|decision\n",
    "Where decision is either 'renew' or 'terminate' but the words in the text might be synonyms of 'renew' or 'terminate'\n",
    "\n",
    "\n",
    "{text}\"\"\"\n",
    "        }],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Parse the simple response\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    client_name, decision = result.split('|')\n",
    "    \n",
    "    decisions_list.append({\n",
    "        \"client\": client_name.strip(),\n",
    "        \"decision\": decision.strip(),\n",
    "        \"source_file\": target_file\n",
    "    })\n",
    "\n",
    "print(\"done processing!\")\n",
    "df = pd.DataFrame(decisions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client</th>\n",
       "      <th>decision</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pen Federal Credit Union</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_01.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Help Desk Inc.</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_02.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global Wax n' Wane</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_03.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kick Box</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_04.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RedKey Inc.</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_05.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clip-n-Chip</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_06.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CoLens Limited</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_07.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Diceware Inc.</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_08.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Teflon Inc.</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_09.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RBG Inc.</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pumkin Spice Corp</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_A.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Camera AI</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_B.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Glassware Design</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_C.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Check Cashing</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_D.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ramp Lamp Corp.</td>\n",
       "      <td>terminate</td>\n",
       "      <td>practice_documents/text_doc_E.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cap-n-Gown Inc.</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_F.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Crooked Curve Association</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_G.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pet Bed Nation Inc.</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_H.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Flower Power</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_I.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Speaker List Bureau</td>\n",
       "      <td>renew</td>\n",
       "      <td>practice_documents/text_doc_J.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       client   decision                         source_file\n",
       "0    Pen Federal Credit Union      renew  practice_documents/text_doc_01.txt\n",
       "1              Help Desk Inc.  terminate  practice_documents/text_doc_02.txt\n",
       "2          Global Wax n' Wane      renew  practice_documents/text_doc_03.txt\n",
       "3                    Kick Box  terminate  practice_documents/text_doc_04.txt\n",
       "4                 RedKey Inc.  terminate  practice_documents/text_doc_05.txt\n",
       "5                 Clip-n-Chip  terminate  practice_documents/text_doc_06.txt\n",
       "6              CoLens Limited  terminate  practice_documents/text_doc_07.txt\n",
       "7               Diceware Inc.      renew  practice_documents/text_doc_08.txt\n",
       "8                 Teflon Inc.      renew  practice_documents/text_doc_09.txt\n",
       "9                    RBG Inc.      renew  practice_documents/text_doc_10.txt\n",
       "10          Pumkin Spice Corp      renew   practice_documents/text_doc_A.txt\n",
       "11                  Camera AI      renew   practice_documents/text_doc_B.txt\n",
       "12           Glassware Design  terminate   practice_documents/text_doc_C.txt\n",
       "13              Check Cashing  terminate   practice_documents/text_doc_D.txt\n",
       "14            Ramp Lamp Corp.  terminate   practice_documents/text_doc_E.txt\n",
       "15            Cap-n-Gown Inc.      renew   practice_documents/text_doc_F.txt\n",
       "16  Crooked Curve Association      renew   practice_documents/text_doc_G.txt\n",
       "17        Pet Bed Nation Inc.      renew   practice_documents/text_doc_H.txt\n",
       "18               Flower Power      renew   practice_documents/text_doc_I.txt\n",
       "19        Speaker List Bureau      renew   practice_documents/text_doc_J.txt"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Confession Judgment Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## glob confession judgment files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our previous ```.txt``` files, these are ```pdf``` files, and PDFs are notoriously obnoxious. They are designed so people can't change them easily. Old packages like ```PyPDF2``` pretty much sucked.\n",
    "\n",
    "But our ability to read PDFs has improved dramatically, or has at least been simplified, because AI companies NEED to unlock their content to train their LLMs. There's ```pymupdf4llm```,   ```PyMuPDF``` and several others.\n",
    "\n",
    "We'll use ```PyMuPDF``` to read these files because it's clean and super fast!\n",
    "\n",
    "```bash\n",
    "pip install PyMuPDF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import PyMuPDF which uses the name 'fitz'\n",
    "```python\n",
    "import fitz  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import fitz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read a single file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AI to code your code here to quantify the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "week-8C-download-and-read_BLANK.ipynb",
   "provenance": [
    {
     "file_id": "1O76yK1JGDPc1kRTHTOIDjev2XbLGExhv",
     "timestamp": 1646785092936
    },
    {
     "file_id": "1U8zDTAuA8NQ2JaDzSqzSn03P2iT8KFLG",
     "timestamp": 1639579061177
    },
    {
     "file_id": "1tC30RwvbvYZQU9bLZ75I6kBbMski9LFN",
     "timestamp": 1628626954402
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
